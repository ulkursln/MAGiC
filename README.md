
## <img src="https://raw.github.com/ulkursln/MAGiC/master/MAGiC/images/icon_shrinked.png" width="52" height="52"/> MAGiC: A Multimodal Framework for Analysing Gaze in Communication 

**MAGiC** is an open source software application, for integrating and analyzing multimodal channels in face-to-face social communication. Specifically, MAGiC integrates gaze data, audio data for speech segmentation and video data for face tracking. Find out more in our [Wiki on GitHub](https://github.com/ulkursln/MAGiC/wiki) and [Youtube Channel](https://www.youtube.com/channel/UC2gvq0OluwpdjVKGSGg-vaQ).

---
<p align="center">
<img src="https://raw.github.com/ulkursln/MAGiC/master/MAGiC/images/AOI.png" width="350"/> <img src="https://raw.github.com/ulkursln/MAGiC/master/MAGiC/images/speechAnnotation.png" width="500"/>
</p>

---

### What it does

Speech and gaze are closely connected modalities in social interaction. MAGiC is a tool for the analysis of social behavior. It enables researchers to overlay gaze data on top of dynamical scene recordings and associates it with the speech information at that moment. MAGiC expands the capacity of current eye tracking technologies by integrating automated face tracking to detect whether a human participant is looking at the interlocutor’s face, and if so, which part of the face is being looked at. Then, it integrates area-of-interest that is relative to the position of interlocutor's face,  with automatically segmented and manuaally annotated speech-data. Specifically, MAGiC provides functionalities for:

* automatic speech and gaze analysis
* visualizing and reviewing outcomes 
* semi-automatically synchronizing multiple recordings
* generates standard output files (i.e., .wav and .txt files) 

---

### WIKI

For instructions of how to install/compile/use the project please see [WIKI](https://github.com/ulkursln/MAGiC/wiki)

---

### Developing and Contributing

We welcome and appreciate contributions from the community. There are many ways to become involved with MAGiC: including filing issues, writing and improving documentation, and contributing to the code. Please keep the following in mind before sending pull request:

* make sure your branch is rebased on the master branch of this repository
* ensure that code is stable enough and does compile.
* explain clearly what the purpose of the patch is, and how you achieved it.

---

### License

[![GPLv3+](http://gplv3.fsf.org/gplv3-127x51.png)](https://github.com/ulkursln/MAGiC/blob/master/LICENSE)
MAGiC is licensed under the GNU General Public License (GPL).
You have to respect [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace), [Sphinx4](https://github.com/cmusphinx/sphinx4), boost, TBB, dlib, and OpenCV licenses.
Thank you!

---

### Reference

@article{Arslan Aydin_Kalkan_Acarturk_2018, \
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; title={MAGiC: A multimodal framework for analysing gaze in dyadic communication}, \
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; volume={11}, \
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; url={https://bop.unibe.ch/JEMR/article/view/4292}, \
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; DOI={10.16910/jemr.11.6.2}, \
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; number={6}, \
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; journal={Journal of Eye Movement Research}, \
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; author={Arslan Aydin, Ülkü and Kalkan, Sinan and Acarturk, Cengiz}, \
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; year={2018}, \
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; month={Nov.} \
&nbsp; &nbsp; &nbsp; &nbsp; }\
 **How to Cite**
    
 * **APA:** Arslan Aydin, Ülkü, Kalkan, S., & Acarturk, C. (2018). MAGiC: A multimodal framework for analysing gaze in dyadic communication. Journal of Eye Movement Research, 11(6). https://doi.org/10.16910/jemr.11.6.2
 * **IEEE** Ülkü Arslan Aydin, S. Kalkan, and C. Acarturk, “MAGiC: A multimodal framework for analysing gaze in dyadic communication”, JEMR, vol. 11, no. 6, Nov. 2018.
